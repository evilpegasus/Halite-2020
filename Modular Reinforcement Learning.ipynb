{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'set_random_seed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-cfd71e11d3aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m123\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_random_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    158\u001b[0m \u001b[0msession_conf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mintra_op_parallelism_threads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minter_op_parallelism_threads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msession_conf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'set_random_seed'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This kernel is a more modular approach to the notebook \"Designing game AI with Reinforcement learning\" by Victor Basu. \n",
    "The objective here was to modify his notebook into a script which could utilize multiple actor/critic models simultaneously.\n",
    "DocStrings have been included for clarity. I'm running this locally on linux with TensorFlow GPU v1.14.\n",
    "\n",
    "\n",
    "I hope this is helpful for those who are exploring reinforcement learning for this year's halite competition. Good luck!\n",
    "\n",
    "\n",
    "UPDATES FROM V3->V4:\n",
    "    -refactoring\n",
    "    -updated DocStrings\n",
    "    -new ship agent\n",
    "    -model trains much faster\n",
    "    -reward vs. episode plot\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "from kaggle_environments import make\n",
    "from kaggle_environments.envs.halite.helpers import *\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np  \n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "\n",
    "tf.enable_eager_execution() # required for TensorFlow v1.14\n",
    "\n",
    "\n",
    "class LOGIC:\n",
    "    def __init__(self, labels: List[str], agent: Callable, board_converter: Callable, rl_model: tf.keras.Model):\n",
    "        \"\"\"\n",
    "        Class for handling a neural net agent. Includes: training, next action generation, and more!\n",
    "\n",
    "        :param labels: string names for possible actions of this agent\n",
    "        :param agent: agent function\n",
    "        :param board_converter: function to convert board to neural net input\n",
    "        :param rl_model: neural net model\n",
    "        \"\"\"\n",
    "        self.optimizer = tf.keras.optimizers.Adam(lr=7e-4)\n",
    "        self.huber_loss = tf.keras.losses.Huber()\n",
    "        self.action_probs_history = list()\n",
    "        self.critic_value_history = list()\n",
    "        self.rewards_history = list()\n",
    "        self.running_reward = 0\n",
    "        self.episode_count = 0\n",
    "        self.num_actions = 5\n",
    "        self.eps = np.finfo(np.float32).eps.item()\n",
    "        self.gamma = 0.99  # Discount factor for past rewards\n",
    "        self.le = preprocessing.LabelEncoder()\n",
    "        self.label_encoded = self.le.fit_transform(labels)\n",
    "        self.agent = agent\n",
    "        self.model = rl_model\n",
    "        self.convert_board = board_converter\n",
    "\n",
    "    def train_step(self, current_board: Board, ship_index: int) -> Union[ShipAction, ShipyardAction]:\n",
    "        \"\"\"\n",
    "        Train model for one time step with provided game board\n",
    "\n",
    "        :param current_board: Board object\n",
    "        :param ship_index: index of ship or shipyard\n",
    "        :return: next action\n",
    "        \"\"\"\n",
    "\n",
    "        model_input = self.convert_board(current_board)\n",
    "        action_prob, critic_value = self.model(model_input)\n",
    "        self.critic_value_history.append(critic_value[0, 0])\n",
    "        current_action = np.random.choice(self.num_actions, p=action_prob.numpy()[0])\n",
    "        self.action_probs_history.append(tf.math.log(action_prob[0, current_action]))\n",
    "        current_action = self.le.inverse_transform([current_action])[0]\n",
    "        return self.agent(board, current_action, ship_index)\n",
    "\n",
    "    def add_gain(self, step_gain: int) -> None:\n",
    "        \"\"\"\n",
    "        append step gain to model\n",
    "\n",
    "        :param step_gain: step gain\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.rewards_history.append(step_gain)\n",
    "\n",
    "    def propagate(self, gradient_tape: tf.GradientTape) -> None:\n",
    "        \"\"\"\n",
    "        Manage reward calculation & back-propagation through network\n",
    "\n",
    "        :param gradient_tape: TensorFlow gradient tape\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "\n",
    "        # Calculate expected value from rewards\n",
    "        # - At each time step what was the total reward received after that timestep\n",
    "        # - Rewards in the past are discounted by multiplying them with gamma\n",
    "        # - These are the labels for our critic\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "\n",
    "        for r in self.rewards_history[::-1]:\n",
    "            discounted_sum = r + self.gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "        # Normalize\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + self.eps)\n",
    "        returns = returns.tolist()\n",
    "        # Calculating loss values to update our network\n",
    "        history = zip(self.action_probs_history, self.critic_value_history, returns)\n",
    "\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            # At this point in history, the critic estimated that we would get a\n",
    "            # total reward = `value` in the future. We took an action with log probability\n",
    "            # of `log_prob` and ended up receiving a total reward = `ret`.\n",
    "            # The actor must be updated so that it predicts an action that leads to\n",
    "            # high rewards (compared to critic's estimate) with high probability.\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "            # The critic must be updated so that it predicts a better estimate of\n",
    "            # the future rewards.\n",
    "            critic_losses.append(\n",
    "                self.huber_loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "            )\n",
    "        # Backpropagation\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = gradient_tape.gradient(loss_value, self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        # Clear the loss and reward history\n",
    "        self.action_probs_history.clear()\n",
    "        self.critic_value_history.clear()\n",
    "        self.rewards_history.clear()\n",
    "\n",
    "    def get_action(self, current_board: Board, ship_index: int) -> Union[ShipAction, ShipyardAction]:\n",
    "        \"\"\"\n",
    "        Generate next action\n",
    "\n",
    "        :param current_board: Board object\n",
    "        :param ship_index: index of ship or shipyard\n",
    "        :return: next action\n",
    "        \"\"\"\n",
    "        model_input = self.convert_board(current_board)\n",
    "        action_prob, _ = self.model(model_input)\n",
    "        current_action = np.random.choice(self.num_actions, p=action_prob.numpy()[0])\n",
    "        current_action = self.le.inverse_transform([current_action])[0]\n",
    "        return self.agent(board, current_action, ship_index)\n",
    "\n",
    "\n",
    "seed = 123\n",
    "tf.set_random_seed(seed)\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "logging.disable(sys.maxsize)\n",
    "global ship_\n",
    "\n",
    "\n",
    "def actor_model(num_actions, in_):\n",
    "    common = tf.keras.layers.Dense(128, activation='tanh')(in_)\n",
    "    common = tf.keras.layers.Dense(32, activation='tanh')(common)\n",
    "    common = tf.keras.layers.Dense(num_actions, activation='softmax')(common)\n",
    "    return common\n",
    "\n",
    "\n",
    "def critic_model(in_):\n",
    "    common = tf.keras.layers.Dense(128)(in_)\n",
    "    common = tf.keras.layers.ReLU()(common)\n",
    "    common = tf.keras.layers.Dense(32)(common)\n",
    "    common = tf.keras.layers.ReLU()(common)\n",
    "    common = tf.keras.layers.Dense(1)(common)\n",
    "    return common\n",
    "\n",
    "\n",
    "input_ = tf.keras.layers.Input(shape=[441, ])\n",
    "model = tf.keras.Model(inputs=input_, outputs=[actor_model(5, input_), critic_model(input_)])\n",
    "print(model.summary())\n",
    "\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "env = make(\"halite\", debug=True)\n",
    "trainer = env.train([None, \"random\"]) # you may have to specify a python file for 'random'\n",
    "\n",
    "\n",
    "def get_dir_to(from_pos, to_pos, size):\n",
    "    from_x, from_y = divmod(from_pos[0], size), divmod(from_pos[1], size)\n",
    "    to_x, to_y = divmod(to_pos[0], size), divmod(to_pos[1], size)\n",
    "    if from_y < to_y:\n",
    "        return ShipAction.NORTH\n",
    "    if from_y > to_y:\n",
    "        return ShipAction.SOUTH\n",
    "    if from_x < to_x:\n",
    "        return ShipAction.EAST\n",
    "    if from_x > to_x:\n",
    "        return ShipAction.WEST\n",
    "\n",
    "\n",
    "# Directions a ship can move\n",
    "directions = [ShipAction.NORTH, ShipAction.EAST, ShipAction.SOUTH, ShipAction.WEST]\n",
    "\n",
    "\n",
    "def decode_dir(act_: str) -> Union[ShipAction, None]:\n",
    "    \"\"\"\n",
    "    Get ShipAction from string\n",
    "\n",
    "    :param act_: string action\n",
    "    :return: ShipAction\n",
    "    \"\"\"\n",
    "\n",
    "    decode = {\n",
    "        'NORTH': ShipAction.NORTH,\n",
    "        'EAST': ShipAction.EAST,\n",
    "        'WEST': ShipAction.WEST,\n",
    "        'SOUTH': ShipAction.SOUTH,\n",
    "        'CONVERT': ShipAction.CONVERT,\n",
    "        'NONE': None\n",
    "    }\n",
    "    return decode[act_]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def advanced_agent(board: Board, action: str, ship_index: int):\n",
    "    # Returns the commands we send to our ships and shipyards\n",
    "    me = board.current_player\n",
    "    act = action\n",
    "\n",
    "    if act == \"CONVERT\" and len(me.ships) / (1 if len(me.shipyards) == 0 else len(me.shipyards)) < 3 and me.ships:\n",
    "        # minimum 3 ships per shipyard\n",
    "        me.ships[ship_index].next_action = None\n",
    "        return me.next_actions\n",
    "\n",
    "    # If there are no ships, use first shipyard to spawn a ship.\n",
    "    if len(me.ships) == 0 and len(me.shipyards) > 0:\n",
    "        me.shipyards[0].next_action = ShipyardAction.SPAWN\n",
    "        return me.next_actions\n",
    "\n",
    "    # If there are no shipyards, convert first ship into shipyard.\n",
    "    if len(me.shipyards) == 0 and len(me.ships) > 0:\n",
    "        me.ships[ship_index].next_action = ShipAction.CONVERT\n",
    "    elif len(me.ships) > 0:\n",
    "        if me.ships[ship_index].halite > 200:\n",
    "            direction = get_dir_to(me.ships[0].position, me.shipyards[0].position, board.configuration.size)\n",
    "            if direction:\n",
    "                me.ships[0].next_action = direction\n",
    "        else:\n",
    "            me.ships[ship_index].next_action = decode_dir(act)\n",
    "    return me.next_actions\n",
    "\n",
    "\n",
    "def convert(board: Board) -> tf.Tensor:\n",
    "    \"\"\"\n",
    "    Extract relevant board/player data and convert to tensor input\n",
    "\n",
    "    :param board: Board object\n",
    "    :return: tensor\n",
    "    \"\"\"\n",
    "    state_ = tf.convert_to_tensor([board.cells[Point(x, y)].halite for x in range(21) for y in range(21)])\n",
    "    state_ = tf.expand_dims(state_, 0)\n",
    "    return state_\n",
    "\n",
    "\n",
    "ship_model = LOGIC(labels=['NORTH', 'SOUTH', 'EAST', 'WEST', 'CONVERT', 'NONE'], agent=advanced_agent,\n",
    "                   board_converter=convert,\n",
    "                   rl_model=model)\n",
    "\n",
    "\n",
    "training = list() # logs reward for each episode\n",
    "\n",
    "while not env.done:\n",
    "    state = trainer.reset()\n",
    "    episode_reward = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        for timestep in tqdm(range(1, env.configuration.episodeSteps + 200)):\n",
    "            board = Board(state, env.configuration)\n",
    "            action = ship_model.train_step(board, 0)\n",
    "            state = trainer.step(action)[0]\n",
    "            gain = state.players[0][0] / 5000\n",
    "            ship_model.add_gain(gain)\n",
    "            episode_reward += gain\n",
    "            if env.done:\n",
    "                state = trainer.reset()\n",
    "                # Update running reward to check condition for solving\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "        training.append([episode_count, running_reward])\n",
    "#         print(\"reward:\", running_reward)\n",
    "        ship_model.propagate(tape)\n",
    "\n",
    "    # Log details\n",
    "    episode_count += 1\n",
    "    if episode_count % 10 == 0:\n",
    "        template = \"running reward: {:.2f} at episode {}\"\n",
    "        print(template.format(running_reward, episode_count))\n",
    "\n",
    "    if running_reward > 550:  # Condition to consider the task solved\n",
    "        print(\"Solved at episode {}!\".format(episode_count))\n",
    "        break\n",
    "#     if episode_count >= 3:\n",
    "#         print(\"max episode reached, training complete!\")\n",
    "#         break\n",
    "\n",
    "# plot reward vs training episode\n",
    "plt.plot([x[0] for x in training], [x[1] for x in training])\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "I use this to generate the halite simulation and run automatically using firefox webdriver, optional\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "out = env.render(mode=\"html\", width=800, height=600)\n",
    "# Write the output to a html file so we can open in a browser.\n",
    "f = open(\"halite.html\", \"w\")\n",
    "f.write(out)\n",
    "f.close()\n",
    "# return\n",
    "driver = webdriver.Firefox()\n",
    "html_file = os.getcwd() + \"//\" + \"halite.html\"\n",
    "driver.get(\"file:///\" + html_file)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
